{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3888205c"
      },
      "outputs": [],
      "source": [
        "!pip install datetime\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install scipy\n",
        "!pip install vaderSentiment\n",
        "!pip install statsmodels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4fa09b11"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import requests\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyldavis"
      ],
      "metadata": {
        "id": "0MGhJbbGrm1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis.gensim"
      ],
      "metadata": {
        "id": "Dw8E8c5mr0FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import gensim\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import nltk; nltk.download('stopwords')\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import re\n",
        "import warnings\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import seaborn as sns\n",
        "%config InlineBackend.figure_formats = ['retina']\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import *\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "rhRVUZ7ur91r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('indextweetsdata.csv')\n",
        "df = df.drop(columns=['Unnamed: 0','Tweet Id', 'Username'])\n",
        "df.rename(columns = {'Datetime':'date','Text':'title'}, inplace = True)\n",
        "df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "df"
      ],
      "metadata": {
        "id": "OynW6kIKtvFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(by = 'date', ascending = True, inplace = True)\n",
        "data_text=df.groupby(['date'])['title'].apply(list).reset_index(name='LDA_data')\n",
        "data_text['index'] = data_text.index\n",
        "document = data_text\n",
        "\n",
        "document"
      ],
      "metadata": {
        "id": "ut1_2s_huA2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['mint', 'india','today','economic','times','indiatimes.com','bloomberg','reuters','moneycontrol','buisiness','nestle','titan','news','com','new','maruti','suzuki',\n",
        "                   'tech','mahindra','business','standard','financial','express','business','insider','ntpc','mahindra','airtel','cnbctv','analytics','insight','sbi','indian','may','reddy','dr','drreddy','say','says','telangana','grid','wipro','vs','pv','magazine','ht','tata','tcs','could','pharma','get','titans'])"
      ],
      "metadata": {
        "id": "7cP_vvtksIG9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
      ],
      "metadata": {
        "id": "L8MpOUDCMrJa"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_newline(series):\n",
        "    return [review.replace('\\n','') for review in series]\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "        \n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n"
      ],
      "metadata": {
        "id": "Kwoyo__MM6Iu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bigrams(words, bi_min=15, tri_min=10):\n",
        "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
        "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "    return bigram_mod\n"
      ],
      "metadata": {
        "id": "zZfc4yYONCel"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_corpus(df):\n",
        "    df = strip_newline(df)\n",
        "    words = list(sent_to_words(df))\n",
        "    words = remove_stopwords(words)\n",
        "    bigram = bigrams(words)\n",
        "    bigram = [bigram[sent] for sent in words]\n",
        "\n",
        "    id2word = gensim.corpora.Dictionary(bigram)\n",
        "    id2word.compactify()\n",
        "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
        "    return corpus, id2word, bigram"
      ],
      "metadata": {
        "id": "RHg4xhscNmcx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "lda_train4=[]\n",
        "train_corpus4=[]\n",
        "for documents in document['LDA_data']:\n",
        "  train_corpus1, train_id2word4, bigram_train4 = get_corpus(documents)\n",
        "  train_corpus4.append(train_corpus1)\n",
        "  with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore')\n",
        "    lda_train4.append(gensim.models.ldamodel.LdaModel(corpus=train_corpus1,num_topics=15,id2word=train_id2word4,passes=5,per_word_topics=True,eval_every = 1))\n"
      ],
      "metadata": {
        "id": "-_C1FUmoqbXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_vecs = []\n",
        "for i in range(len(document['LDA_data'])):\n",
        "  temp=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "  for j in range(len(train_corpus4[i])):\n",
        "    top_topics = lda_train4[i].get_document_topics(train_corpus4[i][j])\n",
        "    for k in range(len(top_topics)):\n",
        "      temp[k]=temp[k]+top_topics[k][1]\n",
        "    \n",
        "  for k in range(15):\n",
        "    temp[k]=temp[k]/len(train_corpus4[i])\n",
        "  train_vecs.append(temp)"
      ],
      "metadata": {
        "id": "VepO-6hbTATS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_vecs[100])) "
      ],
      "metadata": {
        "id": "ZRoxhKI7nywG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install yfinance"
      ],
      "metadata": {
        "id": "p9JLr5aXDefa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "Sensex = yf.download('^BSESN', start= '2020-12-22', end= '2022-12-9')\n",
        "Sensex.reset_index(inplace=True)\n",
        "Sensex['Daily Return'] = (Sensex['Adj Close']/ Sensex['Adj Close'].shift(1)) -1\n",
        "\n",
        "Sensex['DV3'] = ''\n",
        "for i in range(3):\n",
        "    col_name = 'shift'+str(i)\n",
        "    Sensex[col_name] = Sensex['Close'].shift(i)\n",
        "\n",
        "for i in range(Sensex.shape[0]):\n",
        "    Sensex['DV3'][i] = np.std([Sensex[('shift'+str(j))][i] for j in range(3)])\n",
        "    \n",
        "Sensex['DV3-1'] = Sensex['DV3'].shift(-1)\n",
        "\n",
        "Sensex.dropna\n",
        "\n",
        "Sensex['Boolean'] = ''\n",
        "for i in range(2,(Sensex.shape[0]-1)):\n",
        "\n",
        "    if Sensex['DV3'][i]>Sensex['DV3-1'][i]:\n",
        "        Sensex['Boolean'][i]=1\n",
        "    elif Sensex['DV3'][i]<=Sensex['DV3-1'][i]:\n",
        "         Sensex['Boolean'][i]=0\n",
        "\n",
        "Sensex.head(10)"
      ],
      "metadata": {
        "id": "aiNh_s2D3beF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sensex=Sensex.drop(columns=['Open','High','Low','Close','Adj Close','Volume','Daily Return','DV3','shift0','shift1','shift2','DV3-1'])"
      ],
      "metadata": {
        "id": "6mSWn9q946rF"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sensex.reset_index()"
      ],
      "metadata": {
        "id": "WxIjV7eP57rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sensex.columns = ['date','UP_DOWN']\n",
        "Sensex['date'] = pd.to_datetime(Sensex['date']).dt.date"
      ],
      "metadata": {
        "id": "QgPggrwG7r20"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(Sensex, document, on='date', how='inner')\n",
        "merged_df"
      ],
      "metadata": {
        "id": "-Twkh-Ql8Bao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=[]\n",
        "for i in merged_df['index']:\n",
        "    train.append(train_vecs)"
      ],
      "metadata": {
        "id": "qGbYOFJy9Dn2"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array(train)\n",
        "y=np.array(merged_df['UP_DOWN'])"
      ],
      "metadata": {
        "id": "LOAYO5SI9qbq"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(5, shuffle=True, random_state=42)\n",
        "cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
        "cv_lr_a, cv_lrsgd_a, cv_svcsgd_a,  = [], [], []\n",
        "cv_lr_p, cv_lrsgd_p, cv_svcsgd_p,  = [], [], []\n",
        "cv_lr_r, cv_lrsgd_r, cv_svcsgd_r,  = [], [], []\n",
        "\n",
        "for train_ind, val_ind in kf.split(X, y):\n",
        "    X_train, y_train = X[train_ind], y[train_ind]\n",
        "    X_val, y_val = X[val_ind], y[val_ind]\n",
        "\n",
        "    nsamples, nx, ny = X_train.shape\n",
        "    X_train = X_train.reshape((nsamples,nx*ny))\n",
        "\n",
        "    nsamples, nx, ny = X_val.shape\n",
        "    X_val = X_val.reshape((nsamples,nx*ny))\n",
        "\n",
        "    y_train=y_train.astype('int')\n",
        "    y_val=y_val.astype('int')\n",
        "    \n",
        "    scaler = StandardScaler()\n",
        "    X_train_scale = scaler.fit_transform(X_train)\n",
        "    X_val_scale = scaler.transform(X_val)\n",
        "\n",
        "    lr = LogisticRegression(class_weight= 'balanced',solver='newton-cg',fit_intercept=True).fit(X_train_scale, y_train)\n",
        "\n",
        "    y_pred = lr.predict(X_val_scale)\n",
        "    cv_lr_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
        "    cv_lr_p.append(precision_score(y_val, y_pred, average='binary'))\n",
        "    cv_lr_a.append(accuracy_score(y_val, y_pred))\n",
        "    cv_lr_r.append(recall_score(y_val, y_pred, average='binary'))\n",
        "\n",
        "    sgd = linear_model.SGDClassifier(max_iter=1000,tol=1e-3,loss='log',class_weight='balanced').fit(X_train_scale, y_train)\n",
        "    \n",
        "    y_pred = sgd.predict(X_val_scale)\n",
        "    cv_lrsgd_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
        "    cv_lrsgd_p.append(precision_score(y_val, y_pred, average='binary'))\n",
        "    cv_lrsgd_a.append(accuracy_score(y_val, y_pred))\n",
        "    cv_lrsgd_r.append(recall_score(y_val, y_pred, average='binary'))\n",
        "    \n",
        "    sgd_huber = linear_model.SGDClassifier(max_iter=1000,tol=1e-3,alpha=20,loss='modified_huber',class_weight='balanced').fit(X_train_scale, y_train)\n",
        "    \n",
        "    y_pred = sgd_huber.predict(X_val_scale)\n",
        "    cv_svcsgd_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
        "    cv_svcsgd_p.append(precision_score(y_val, y_pred, average='binary'))\n",
        "    cv_svcsgd_a.append(accuracy_score(y_val, y_pred))\n",
        "    cv_svcsgd_r.append(recall_score(y_val, y_pred, average='binary'))\n",
        "\n",
        "print('\\n')\n",
        "print(f'Logistic Regression Val f1:            {np.mean(cv_lr_f1):.3f} +- {np.std(cv_lr_f1):.3f}')\n",
        "print(f'Logistic Regression Val Accuracy:      {np.mean(cv_lr_a):.3f} +- {np.std(cv_lr_a):.3f}')\n",
        "print(f'Logistic Regression Val Recall:        {np.mean(cv_lr_r):.3f} +- {np.std(cv_lr_r):.3f}')\n",
        "print(f'Logistic Regression Val Precision:     {np.mean(cv_lr_p):.3f} +- {np.std(cv_lr_p):.3f}')\n",
        "print('\\n')\n",
        "print(f'Logistic Regression SGD Val f1:        {np.mean(cv_lrsgd_f1):.3f} +- {np.std(cv_lrsgd_f1):.3f}')\n",
        "print(f'Logistic Regression SGD Val Accuracy:  {np.mean(cv_lrsgd_a):.3f} +- {np.std(cv_lrsgd_a):.3f}')\n",
        "print(f'Logistic Regression SGD Val Recall:    {np.mean(cv_lrsgd_r):.3f} +- {np.std(cv_lrsgd_r):.3f}')\n",
        "print(f'Logistic Regression SGD Val Precision: {np.mean(cv_lrsgd_p):.3f} +- {np.std(cv_lrsgd_p):.3f}')\n",
        "print('\\n')\n",
        "print(f'SVM Huber Val f1:                      {np.mean(cv_svcsgd_f1):.3f} +- {np.std(cv_svcsgd_f1):.3f}')\n",
        "print(f'SVM Huber Val Accuracy:                {np.mean(cv_svcsgd_a):.3f} +- {np.std(cv_svcsgd_a):.3f}')\n",
        "print(f'SVM Huber Val Recall:                  {np.mean(cv_svcsgd_r):.3f} +- {np.std(cv_svcsgd_r):.3f}')\n",
        "print(f'SVM Huber Val Precision:               {np.mean(cv_svcsgd_p):.3f} +- {np.std(cv_svcsgd_p):.3f}')\n",
        "print('\\n')\n"
      ],
      "metadata": {
        "id": "OuwE1tTX-Mgx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}